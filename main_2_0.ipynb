{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Comments\n",
    "Thanks to tito for this great script https://www.kaggle.com/its7171/lgbm-with-loop-feature-engineering\n",
    "\n",
    "* Creating predictive feature is very important, here I just used 14 features and 15M data points to train the model.\n",
    "* The dataset is big to preprocess using python with a for loop, their are other tools and frameworks like (SQL, Spark, Apache Beam, Dask) where you could make feature engineering much faster but if we are smart and make predictive feature it's ok to just use for loops.\n",
    "* Foward feature engineering seems a good technique to try in this problem (create 1 new feature that you think it could be predective based on the problem, run the pipeline and check if val score increase, if it increase that feature is predictive and you should add it. Care when you just get some minor improvement, sometime is better to discard that feature because your experimentation process is going to get slower)."
   ]
  },
  {
   "metadata": {
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "trusted": true
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gc\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from collections import defaultdict\n",
    "from tqdm.notebook import tqdm\n",
    "import lightgbm as lgb\n",
    "import riiideducation\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import random\n",
    "import os"
   ],
   "execution_count": 1,
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "<style type='text/css'>\n.datatable table.frame { margin-bottom: 0; }\n.datatable table.frame thead { border-bottom: none; }\n.datatable table.frame tr.coltypes td {  color: #FFFFFF;  line-height: 6px;  padding: 0 0.5em;}\n.datatable .bool    { background: #DDDD99; }\n.datatable .object  { background: #565656; }\n.datatable .int     { background: #5D9E5D; }\n.datatable .float   { background: #4040CC; }\n.datatable .str     { background: #CC4040; }\n.datatable .row_index {  background: var(--jp-border-color3);  border-right: 1px solid var(--jp-border-color0);  color: var(--jp-ui-font-color3);  font-size: 9px;}\n.datatable .frame tr.coltypes .row_index {  background: var(--jp-border-color0);}\n.datatable th:nth-child(2) { padding-left: 12px; }\n.datatable .hellipsis {  color: var(--jp-cell-editor-border-color);}\n.datatable .vellipsis {  background: var(--jp-layout-color0);  color: var(--jp-cell-editor-border-color);}\n.datatable .na {  color: var(--jp-cell-editor-border-color);  font-size: 80%;}\n.datatable .footer { font-size: 9px; }\n.datatable .frame_dimensions {  background: var(--jp-border-color3);  border-top: 1px solid var(--jp-border-color0);  color: var(--jp-ui-font-color3);  display: inline-block;  opacity: 0.6;  padding: 1px 10px 1px 5px;}\n</style>\n"
     },
     "metadata": {}
    }
   ]
  },
  {
   "metadata": {
    "trusted": true
   },
   "cell_type": "code",
   "source": [
    "# Random seed\n",
    "SEED = 123\n",
    "\n",
    "# Function to seed everything\n",
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    \n",
    "seed_everything(SEED)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "trusted": true
   },
   "cell_type": "code",
   "source": [
    "def read_and_preprocess(feature_engineering = False):\n",
    "    \n",
    "    train_pickle = '../input/riiid-cross-validation-files/cv1_train.pickle'\n",
    "    valid_pickle = '../input/riiid-cross-validation-files/cv1_valid.pickle'\n",
    "    question_file = '../input/riiid-test-answer-prediction/questions.csv'\n",
    "    \n",
    "    # Read data\n",
    "    feld_needed = ['timestamp', 'user_id', 'answered_correctly', 'content_id', 'content_type_id', 'prior_question_elapsed_time', 'prior_question_had_explanation']\n",
    "    train = pd.read_pickle(train_pickle)[feld_needed]\n",
    "    valid = pd.read_pickle(valid_pickle)[feld_needed]\n",
    "    # Delete some trianing data to don't have ram problems\n",
    "    if feature_engineering:\n",
    "        train = train.iloc[-40000000:]\n",
    "    \n",
    "    # Filter by content_type_id to discard lectures\n",
    "    train = train.loc[train.content_type_id == False].reset_index(drop = True)\n",
    "    valid = valid.loc[valid.content_type_id == False].reset_index(drop = True)\n",
    "    \n",
    "    # Changing dtype to avoid lightgbm error\n",
    "    train['prior_question_had_explanation'] = train.prior_question_had_explanation.fillna(False).astype('int8')\n",
    "    valid['prior_question_had_explanation'] = valid.prior_question_had_explanation.fillna(False).astype('int8')\n",
    "    \n",
    "    # Fill prior question elapsed time with the mean\n",
    "    prior_question_elapsed_time_mean = train['prior_question_elapsed_time'].dropna().mean()\n",
    "    train['prior_question_elapsed_time'].fillna(prior_question_elapsed_time_mean, inplace = True)\n",
    "    valid['prior_question_elapsed_time'].fillna(prior_question_elapsed_time_mean, inplace = True)\n",
    "    \n",
    "    # Merge with question dataframe\n",
    "    questions_df = pd.read_csv(question_file)\n",
    "    questions_df['part'] = questions_df['part'].astype(np.int32)\n",
    "    questions_df['bundle_id'] = questions_df['bundle_id'].astype(np.int32)\n",
    "    \n",
    "    train = pd.merge(train, questions_df[['question_id', 'part']], left_on = 'content_id', right_on = 'question_id', how = 'left')\n",
    "    valid = pd.merge(valid, questions_df[['question_id', 'part']], left_on = 'content_id', right_on = 'question_id', how = 'left')\n",
    "\n",
    "    answered_correctly_u_count_dict = defaultdict(int)\n",
    "    answered_correctly_u_sum_dict = defaultdict(int)\n",
    "    answered_correctly_uq_dict = defaultdict(lambda: defaultdict(int))\n",
    "\n",
    "    elapsed_time_u_sum_dict = defaultdict(int)\n",
    "    explanation_u_sum_dict = defaultdict(int)\n",
    "    question_u_count_dict = defaultdict(int)\n",
    "    question_u_last_bundle_count_dict = defaultdict(int)\n",
    "\n",
    "    part_user_count_dict = defaultdict(lambda: defaultdict(int))\n",
    "    part_user_sum_dict = defaultdict(lambda: defaultdict(int))\n",
    "\n",
    "    question_correct_last_20_count_dict = defaultdict(int)\n",
    "    question_correct_last_20_sum_dict = defaultdict(int)\n",
    "    question_correct_last_20_all_dict = defaultdict(list)\n",
    "\n",
    "    timestamp_u_correct_dict = defaultdict(list)\n",
    "    timestamp_u_incorrect_dict = defaultdict(list)\n",
    "\n",
    "    timestamp_u_dict = defaultdict(list)\n",
    "\n",
    "    user_tag_acc_count_dict = defaultdict(lambda: defaultdict(int))\n",
    "    user_tag_acc_sum_dict = defaultdict(lambda: defaultdict(int))\n",
    "    \n",
    "    # Client Question dictionary\n",
    "    answered_correctly_uq = defaultdict(lambda: defaultdict(int))\n",
    "\n",
    "    # Client Question dictionary\n",
    "    answered_correctly_uq = defaultdict(lambda: defaultdict(int))\n",
    "    \n",
    "    print('User feature calculation started...')\n",
    "    print('\\n')\n",
    "    train = add_features(train)\n",
    "    valid = add_features(valid)\n",
    "    gc.collect()\n",
    "    print('User feature calculation completed...')\n",
    "    print('\\n')\n",
    "    \n",
    "    features_dicts = {\n",
    "        'answered_correctly_u_count_dict': answered_correctly_u_count_dict,\n",
    "        'answered_correctly_u_sum_dict': answered_correctly_u_sum_dict,\n",
    "        'answered_correctly_uq_dict': answered_correctly_uq_dict,\n",
    "        'elapsed_time_u_sum_dict': elapsed_time_u_sum_dict,\n",
    "        'explanation_u_sum_dict': explanation_u_sum_dict,\n",
    "        'question_u_count_dict': question_u_count_dict,\n",
    "        'question_u_last_bundle_count_dict': question_u_last_bundle_count_dict,\n",
    "        'part_user_count_dict': part_user_count_dict,\n",
    "        'part_user_sum_dict': part_user_sum_dict,\n",
    "        'question_correct_last_20_count_dict': question_correct_last_20_count_dict,\n",
    "        'question_correct_last_20_sum_dict': question_correct_last_20_sum_dict\n",
    "        'question_correct_last_20_all_dict': question_correct_last_20_all_dict,\n",
    "        'timestamp_u_incorrect_dict': timestamp_u_incorrect_dict,\n",
    "        'timestamp_u_correct_dict': timestamp_u_correct_dict,\n",
    "        'timestamp_u_dict': timestamp_u_dict,\n",
    "        'user_tag_acc_count_dict': user_tag_acc_count_dict,\n",
    "        'user_tag_acc_sum_dict': user_tag_acc_sum_dict,\n",
    "        'answered_correctly_uq': answered_correctly_uq,\n",
    "    }\n",
    "    \n",
    "    return train, valid, questions_df, prior_question_elapsed_time_mean, features_dicts\n",
    "\n",
    "\n",
    "# Function for user stats with loops\n",
    "def add_features(df, update=True):\n",
    "    global answered_correctly_u_count_dict\n",
    "    global answered_correctly_u_sum_dict\n",
    "    global answered_correctly_uq_dict\n",
    "    global elapsed_time_u_sum_dict\n",
    "    global explanation_u_sum_dict\n",
    "    global question_u_count_dict\n",
    "    global question_u_last_bundle_count_dict\n",
    "    global part_user_count_dict\n",
    "    global part_user_sum_dict\n",
    "    global question_correct_last_20_count_dict\n",
    "    global question_correct_last_20_sum_dict\n",
    "    global question_correct_last_20_all_dict\n",
    "    global timestamp_u_correct_dict\n",
    "    global timestamp_u_incorrect_dict\n",
    "    global timestamp_u_dict\n",
    "    global user_tag_acc_count_dict\n",
    "    global user_tag_acc_sum_dict\n",
    "    global answered_correctly_uq\n",
    "\n",
    "    # Client features\n",
    "    answered_correctly_u_avg = np.zeros(len(df), dtype = np.float32)\n",
    "    answered_correctly_u_count = np.zeros(len(df), dtype = np.float32)\n",
    "    answered_correctly_uq_count = np.zeros(len(df), dtype = np.int32)\n",
    "\n",
    "    elapsed_time_u_avg = np.zeros(len(df), dtype = np.float32)\n",
    "    explanation_u_avg = np.zeros(len(df), dtype = np.float32)\n",
    "\n",
    "    part_user_count = np.zeros(len(df), dtype = np.float32)\n",
    "    part_user_mean = np.zeros(len(df), dtype = np.float32)\n",
    "\n",
    "    question_correct_rate_last_20_sum = np.zeros(len(df), dtype = np.float32)\n",
    "\n",
    "    timestamp_u_correct_recency_1 = np.zeros(len(df), dtype = np.float32)\n",
    "    timestamp_u_incorrect_recency_1 = np.zeros(len(df), dtype = np.float32)\n",
    "\n",
    "    timestamp_u_diff_1 = np.zeros(len(df), dtype = np.float32)\n",
    "    timestamp_u_diff_2 = np.zeros(len(df), dtype = np.float32)\n",
    "    timestamp_u_diff_3 = np.zeros(len(df), dtype = np.float32)\n",
    "\n",
    "    user_tag_acc_count = np.zeros(len(df), dtype = np.float32)\n",
    "    user_tag_acc_max = np.zeros(len(df), dtype = np.float32)\n",
    "    user_tag_acc_min = np.zeros(len(df), dtype = np.float32)\n",
    "\n",
    "    list_last_user_task_table=[]####定义数组 用来保存旧组的信息\n",
    "    list_last_user_task_table_un_back=[]####定义数组 用来保存旧组的信息\n",
    "#     for num, row in enumerate(tqdm(df[['user_id', 'answered_correctly', 'content_id', 'prior_question_elapsed_time', 'prior_question_had_explanation', 'timestamp','task_container_id']].values)):\n",
    "    flag_current_task=0\n",
    "    df_temp=df[['user_id',\"task_container_id\", 'content_id', 'answered_correctly', 'prior_question_elapsed_time', 'prior_question_had_explanation', 'timestamp','part',\"tags\"]].values\n",
    "    for num in tqdm(range(len(df))):\n",
    "        row=df_temp[num]\n",
    "        if num+1!=len(df):\n",
    "            row2=df_temp[num+1]\n",
    "        else:\n",
    "            row2=[-100 for i in range(len(row))]\n",
    "\n",
    "\n",
    "        ####*********  elapsed_time_u_avg_xiuzheng和explanation_u_avg_xiuzheng\n",
    "        if row[6]!=0:##如果时间戳不是0的时候\n",
    "            if flag_current_task==0:\n",
    "                question_u_count_dict[row[0]]+=question_u_last_bundle_count_dict[row[0]]\n",
    "                elapsed_time_u_sum_dict[row[0]]+=row[4]*question_u_last_bundle_count_dict[row[0]]\n",
    "                explanation_u_sum_dict[row[0]]+=row[5]*question_u_last_bundle_count_dict[row[0]]\n",
    "            elapsed_time_u_avg[num]= elapsed_time_u_sum_dict[row[0]]/question_u_count_dict[row[0]]\n",
    "            explanation_u_avg[num] = explanation_u_sum_dict[row[0]]/question_u_count_dict[row[0]]\n",
    "            ###⑥只需要当前组的prior（也就是上一组的平均时间或者是否解答），就可以计算了\n",
    "        else:##时间戳为0的时候，肯定是不知道当前组的用时和解答情况的\n",
    "            elapsed_time_u_avg[num]=np.nan\n",
    "            explanation_u_avg[num] = np.nan\n",
    "        flag_current_task=1\n",
    "\n",
    "        ###①求这个特征，需要不断的记录上一组一共有多少道题，到最后用    （不断累加（每组多少道题*每道题平均时间））/总做题次数\n",
    "        ###②需要把记录这组有多少道题放在后面计算，在前面计算平均时间并且填充到特征数组里\n",
    "        list_last_user_task_table_un_back.append([row[0]])###没换人换组的时候，先不断保存旧组的信息,并且在换人换组的时候也要保存，以防那次信息没被用到\n",
    "        if row[0]!=row2[0] or row[1]!=row2[1]:###换了一个task\n",
    "            flag_current_task=0\n",
    "            question_u_last_bundle_count_dict[row[0]]=len(list_last_user_task_table_un_back)\n",
    "            list_last_user_task_table_un_back=[]###在即将换task的时候，把旧组需要换成新组（更换成新组之前，需要先把旧组的信息在上面用完）\n",
    "\n",
    "        ####*********\n",
    "\n",
    "        ####*********   answered_correctly_u_avg、answered_correctly_u_count和answered_correctly_uq_count\n",
    "        if answered_correctly_u_count_dict[row[0]] != 0:\n",
    "            answered_correctly_u_avg[num] = answered_correctly_u_sum_dict[row[0]] / answered_correctly_u_count_dict[row[0]]\n",
    "            answered_correctly_u_count[num] = answered_correctly_u_count_dict[row[0]]\n",
    "        else:\n",
    "            answered_correctly_u_avg[num] = 0.67\n",
    "            answered_correctly_u_count[num] = 0\n",
    "\n",
    "        answered_correctly_uq_count[num] = answered_correctly_uq_dict[row[0]][row[2]]\n",
    "        ####*********\n",
    "\n",
    "        ####*********   part_user_count和part_user_mean\n",
    "        if part_user_count_dict[row[0]][row[7]]==0:\n",
    "            part_user_count[num] = 0\n",
    "            part_user_mean[num] = 0.67\n",
    "        else:\n",
    "            part_user_count[num] = part_user_count_dict[row[0]][row[7]]\n",
    "            part_user_mean[num] = part_user_sum_dict[row[0]][row[7]]/part_user_count_dict[row[0]][row[7]]\n",
    "        ####*********\n",
    "\n",
    "        ####*********   question_correct_rate_last_20_mean\n",
    "#         question_correct_rate_last_20_sum[num]=question_correct_last_20_sum_dict[row[0]]\n",
    "        ####*********\n",
    "\n",
    "\n",
    "        ####*********   timestamp_u_correct_recency_1，timestamp_u_incorrect_recency_1\n",
    "        if len(timestamp_u_correct_dict[row[0]]) == 0:\n",
    "            timestamp_u_correct_recency_1[num] = np.nan\n",
    "        elif len(timestamp_u_correct_dict[row[0]]) == 1:\n",
    "            timestamp_u_correct_recency_1[num] = row[6] - timestamp_u_correct_dict[row[0]][0]\n",
    "\n",
    "        if len(timestamp_u_incorrect_dict[row[0]]) == 0:\n",
    "            timestamp_u_incorrect_recency_1[num] = np.nan\n",
    "        elif len(timestamp_u_incorrect_dict[row[0]]) == 1:\n",
    "            timestamp_u_incorrect_recency_1[num] = row[6] - timestamp_u_incorrect_dict[row[0]][0]\n",
    "        ####*********\n",
    "\n",
    "        ####*********   timestamp_u_diff_1_2，timestamp_u_diff_2_3，timestamp_u_diff_3_end\n",
    "        if len(timestamp_u_dict[row[0]]) == 0:\n",
    "            timestamp_u_diff_1[num] = np.nan\n",
    "            timestamp_u_diff_2[num] = np.nan\n",
    "            timestamp_u_diff_3[num] = np.nan\n",
    "        elif len(timestamp_u_dict[row[0]]) == 1:\n",
    "            timestamp_u_diff_1[num] = row[6] - timestamp_u_dict[row[0]][0]\n",
    "            timestamp_u_diff_2[num] = np.nan\n",
    "            timestamp_u_diff_3[num] = np.nan\n",
    "        elif len(timestamp_u_dict[row[0]]) == 2:\n",
    "            timestamp_u_diff_1[num] = row[6] - timestamp_u_dict[row[0]][1]\n",
    "            timestamp_u_diff_2[num] = timestamp_u_dict[row[0]][1] - timestamp_u_dict[row[0]][0]\n",
    "            timestamp_u_diff_3[num] = np.nan\n",
    "        elif len(timestamp_u_dict[row[0]]) == 3:\n",
    "            timestamp_u_diff_1[num] = row[6] - timestamp_u_dict[row[0]][2]\n",
    "            timestamp_u_diff_2[num] = timestamp_u_dict[row[0]][2] - timestamp_u_dict[row[0]][1]\n",
    "            timestamp_u_diff_3[num] = timestamp_u_dict[row[0]][1] - timestamp_u_dict[row[0]][0]\n",
    "\n",
    "        ####*********\n",
    "\n",
    "        ####*********   user_tag_acc_count，user_tag_acc_max，user_tag_acc_min\n",
    "        if pd.isnull(row[8]):\n",
    "            user_tag_acc_count[num]=np.nan\n",
    "            user_tag_acc_max[num] = np.nan\n",
    "            user_tag_acc_min[num] = np.nan\n",
    "            continue\n",
    "        else:\n",
    "            tag_list_un_back=row[8].split()\n",
    "            row_all_tag_sum=0\n",
    "            row_all_tag_count=0\n",
    "            row_max_tag_mean=-1###尽量搞小\n",
    "            row_min_tag_mean=1000###尽量搞大\n",
    "\n",
    "            for single_tag in tag_list_un_back:\n",
    "                ###先做需要更新的###\n",
    "                single_tag_sum=user_tag_acc_sum_dict[row[0]][single_tag]\n",
    "                single_tag_count=user_tag_acc_count_dict[row[0]][single_tag]\n",
    "                row_all_tag_sum+=single_tag_sum\n",
    "                row_all_tag_count+=single_tag_count\n",
    "                if single_tag_count==0:\n",
    "                    single_tag_mean=0.67\n",
    "                else:\n",
    "                    single_tag_mean=single_tag_sum/single_tag_count\n",
    "                row_max_tag_mean=max(single_tag_mean,row_max_tag_mean)\n",
    "                row_min_tag_mean=min(single_tag_mean,row_min_tag_mean)\n",
    "            if row_all_tag_count==0:\n",
    "                user_tag_acc_count[num]=0\n",
    "                user_tag_acc_max[num] = 0.67\n",
    "                user_tag_acc_min[num] = 0.67\n",
    "            else:\n",
    "                user_tag_acc_count[num]=row_all_tag_count\n",
    "                user_tag_acc_max[num] = row_max_tag_mean\n",
    "                user_tag_acc_min[num] = row_min_tag_mean\n",
    "        ####*********\n",
    "\n",
    "        if update:\n",
    "            answered_correctly_u_count_dict[row[0]] += 1\n",
    "            answered_correctly_u_sum_dict[row[0]] += row[3]\n",
    "            answered_correctly_uq_dict[row[0]][row[2]] += 1\n",
    "            part_user_count_dict[row[0]][row[7]] += 1\n",
    "            part_user_sum_dict[row[0]][row[7]] += row[3]\n",
    "#             if question_correct_last_20_count_dict[row[0]]+1<=20:\n",
    "#                 question_correct_last_20_count_dict[row[0]]+=1\n",
    "#                 question_correct_last_20_sum_dict[row[0]]+=row[3]\n",
    "#                 question_correct_last_20_all_dict[row[0]].append(row[3])\n",
    "#             else:\n",
    "#                 question_correct_last_20_sum_dict[row[0]]+=row[3]\n",
    "#                 question_correct_last_20_sum_dict[row[0]]-=question_correct_last_20_all_dict[row[0]][-1]\n",
    "#                 question_correct_last_20_all_dict[row[0]].pop(0)\n",
    "#                 question_correct_last_20_all_dict[row[0]].append(row[3])\n",
    "\n",
    "            tag_list=row[8].split()\n",
    "            for single_tag in tag_list:\n",
    "                ######更新一下 user-tag\n",
    "                user_tag_acc_count_dict[row[0]][single_tag] += 1\n",
    "                user_tag_acc_sum_dict[row[0]][single_tag] += row[3]\n",
    "\n",
    "            #'user_id',\"task_container_id\", 'content_id', 'answered_correctly', 'prior_question_elapsed_time', 'prior_question_had_explanation', 'timestamp','part'\n",
    "            list_last_user_task_table.append([row[0],row[1],row[2],row[3],row[4],row[5],row[6],row[7]])###没换人换组的时候，先不断保存旧组的信息,并且在换人换组的时候也要保存，以防那次信息没被用到\n",
    "            if row[0]!=row2[0] or row[1]!=row2[1]:###换了一个task\n",
    "\n",
    "                if len(timestamp_u_dict[row[0]]) == 3:\n",
    "                    timestamp_u_dict[row[0]].pop(0)\n",
    "                    timestamp_u_dict[row[0]].append(row[6])\n",
    "                else:\n",
    "                    timestamp_u_dict[row[0]].append(row[6])\n",
    "\n",
    "                ####由于bundle下面包含很多question，每个question都有一个correct，所以需要用列表存储“旧的一整个组”的correct\n",
    "                for single_row_last_user_task_table in list_last_user_task_table:\n",
    "                    if single_row_last_user_task_table[3]==1:\n",
    "                        if len(timestamp_u_correct_dict[row[0]]) == 1:###这里，就使用row[0]就行，因为list_last_user_task_que_timestamp里全都是当前user-task的信息，而非下一个user-task的信息\n",
    "                            timestamp_u_correct_dict[row[0]].pop(0)\n",
    "                            timestamp_u_correct_dict[row[0]].append(single_row_last_user_task_table[6])\n",
    "                        else:\n",
    "                            timestamp_u_correct_dict[row[0]].append(single_row_last_user_task_table[6])\n",
    "                    else:\n",
    "                        if len(timestamp_u_incorrect_dict[row[0]]) == 1:###这里，就使用row[0]就行，因为list_last_user_task_que_timestamp里全都是当前user-task的信息，而非下一个user-task的信息\n",
    "                            timestamp_u_incorrect_dict[row[0]].pop(0)\n",
    "                            timestamp_u_incorrect_dict[row[0]].append(single_row_last_user_task_table[6])\n",
    "                        else:\n",
    "                            timestamp_u_incorrect_dict[row[0]].append(single_row_last_user_task_table[6])\n",
    "                list_last_user_task_table=[]###在即将换task的时候，把旧组需要换成新组（更换成新组之前，需要先把旧组的信息在上面用完）\n",
    "\n",
    "    df['answered_correctly_u_avg']=answered_correctly_u_avg\n",
    "    df['answered_correctly_u_count']=answered_correctly_u_count\n",
    "    df['answered_correctly_uq_count']=answered_correctly_uq_count\n",
    "    df['elapsed_time_u_avg_xiuzheng']=elapsed_time_u_avg\n",
    "    df['explanation_u_avg_xiuzheng']=explanation_u_avg\n",
    "    df['part_user_count']=part_user_count\n",
    "    df['part_user_mean']=part_user_mean\n",
    "    df['timestamp_u_correct_recency_1']=timestamp_u_correct_recency_1\n",
    "    df['timestamp_u_incorrect_recency_1']=timestamp_u_incorrect_recency_1\n",
    "    df['timestamp_u_diff_1_2']=timestamp_u_diff_1\n",
    "    df['timestamp_u_diff_2_3']=timestamp_u_diff_2\n",
    "    df['timestamp_u_diff_3_end']=timestamp_u_diff_3\n",
    "    df['part_user_count']=part_user_count\n",
    "    df['part_user_mean']=part_user_mean\n",
    "    df['user_tag_acc_count']=user_tag_acc_count\n",
    "    df['user_tag_acc_max']=user_tag_acc_max\n",
    "    df['user_tag_acc_min']=user_tag_acc_min\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "# Function for training and evaluation\n",
    "def train_and_evaluate(train, valid):\n",
    "    \n",
    "    TARGET = 'answered_correctly'\n",
    "    # Features to train and predict\n",
    "    FEATURES = [\n",
    "        'answered_correctly_u_avg',\n",
    "        'answered_correctly_u_count',\n",
    "        'answered_correctly_uq_count',\n",
    "        'elapsed_time_u_avg_xiuzheng',\n",
    "        'explanation_u_avg_xiuzheng',\n",
    "        'part_correctly_q_mean',###线下\n",
    "        'part_elapsed_time_mean',###线下\n",
    "        'part_had_explanation_mean',###线下\n",
    "        'part_user_count',\n",
    "        'part_user_mean',\n",
    "        'prior_question_elapsed_time',###原始\n",
    "        'prior_question_had_explanation',###原始\n",
    "        'question_correct_rate_last_20_mean',\n",
    "        'question_correctly_q_count',###线下\n",
    "        'question_correctly_q_mean',###线下\n",
    "        'question_elapsed_time_mean',###线下\n",
    "        'question_had_explanation_mean',###线下\n",
    "        'tag_acc_count',###线下\n",
    "        'tag_acc_max',###线下\n",
    "        'tag_acc_min',###线下\n",
    "        'tags_lsi',###线下\n",
    "        'task_container_id',###原始\n",
    "        'timestamp',###原始\n",
    "        'timestamp_u_correct_recency_1',\n",
    "        'timestamp_u_diff_1_2',\n",
    "        'timestamp_u_diff_2_3',\n",
    "        'timestamp_u_diff_3_end',\n",
    "        'timestamp_u_incorrect_recency_1',\n",
    "        'user_tag_acc_count',\n",
    "        'user_tag_acc_max',\n",
    "        'user_tag_acc_min'\n",
    "    ]\n",
    "    \n",
    "    # Delete some training data to experiment faster\n",
    "    if False:\n",
    "        train = train.sample(15000000, random_state = SEED)\n",
    "\n",
    "    gc.collect()\n",
    "    print(f'Traning with {train.shape[0]} rows and {len(FEATURES)} features')    \n",
    "    drop_cols = list(set(train.columns) - set(FEATURES))\n",
    "    y_train = train[TARGET]\n",
    "    y_val = valid[TARGET]\n",
    "    # Drop unnecessary columns\n",
    "    train.drop(drop_cols, axis = 1, inplace = True)\n",
    "    valid.drop(drop_cols, axis = 1, inplace = True)\n",
    "    gc.collect()\n",
    "\n",
    "\n",
    "    model = lgb.LGBMClassifier(num_leaves=300,\n",
    "                            max_depth=15,\n",
    "                            learning_rate=0.1,\n",
    "                            subsample=0.8,\n",
    "                            feature_fraction=0.8,\n",
    "                            random_state=2020,\n",
    "                            n_estimators=200\n",
    "                            )\n",
    "    lgb_model = model.fit(train[FEATURES],\n",
    "                        y_train,\n",
    "                        eval_names=['train', 'valid'],\n",
    "                        eval_set=[(train[FEATURES], y_train), (valid[FEATURES], y_val)],\n",
    "                        verbose=10,\n",
    "                        eval_metric='auc',\n",
    "                        early_stopping_rounds=10,\n",
    "                         categorical_feature=['tags_lsi'])\n",
    "    \n",
    "    print('Our Roc Auc score for the validation data is:', roc_auc_score(y_val, lgb_model.predict(valid[FEATURES])))\n",
    "\n",
    "    feature_importance = lgb_model.feature_importance()\n",
    "    feature_importance = pd.DataFrame({'Features': FEATURES, 'Importance': feature_importance}).sort_values('Importance', ascending = False)\n",
    "    \n",
    "    fig = plt.figure(figsize = (10, 10))\n",
    "    fig.suptitle('Feature Importance', fontsize = 20)\n",
    "    plt.tick_params(axis = 'x', labelsize = 12)\n",
    "    plt.tick_params(axis = 'y', labelsize = 12)\n",
    "    plt.xlabel('Importance', fontsize = 15)\n",
    "    plt.ylabel('Features', fontsize = 15)\n",
    "    sns.barplot(x = feature_importance['Importance'], y = feature_importance['Features'], orient = 'h')\n",
    "    plt.show()\n",
    "    \n",
    "    return TARGET, FEATURES, lgb_model\n",
    "\n",
    "# Using time series api that simulates production predictions\n",
    "def inference(TARGET, FEATURES, model, questions_df, prior_question_elapsed_time_mean, features_dicts):\n",
    "    \n",
    "    # Get feature dict\n",
    "    answered_correctly_u_count = features_dicts['answered_correctly_u_count']\n",
    "    answered_correctly_u_sum = features_dicts['answered_correctly_u_sum']\n",
    "    elapsed_time_u_sum = features_dicts['elapsed_time_u_sum']\n",
    "    explanation_u_sum = features_dicts['explanation_u_sum']\n",
    "    answered_correctly_q_count = features_dicts['answered_correctly_q_count']\n",
    "    answered_correctly_q_sum = features_dicts['answered_correctly_q_sum']\n",
    "    elapsed_time_q_sum = features_dicts['elapsed_time_q_sum']\n",
    "    explanation_q_sum = features_dicts['explanation_q_sum']\n",
    "    answered_correctly_uq = features_dicts['answered_correctly_uq']\n",
    "    timestamp_u = features_dicts['timestamp_u']\n",
    "    timestamp_u_incorrect = features_dicts['timestamp_u_incorrect']\n",
    "    \n",
    "    # Get api iterator and predictor\n",
    "    env = riiideducation.make_env()\n",
    "    iter_test = env.iter_test()\n",
    "    set_predict = env.predict\n",
    "    \n",
    "    previous_test_df = None\n",
    "    for (test_df, sample_prediction_df) in iter_test:\n",
    "        if previous_test_df is not None:\n",
    "            previous_test_df[TARGET] = eval(test_df[\"prior_group_answers_correct\"].iloc[0])\n",
    "            update_features(previous_test_df, answered_correctly_u_sum, answered_correctly_q_sum, timestamp_u_incorrect)\n",
    "        previous_test_df = test_df.copy()\n",
    "        test_df = test_df[test_df['content_type_id'] == 0].reset_index(drop = True)\n",
    "        test_df['prior_question_had_explanation'] = test_df.prior_question_had_explanation.fillna(False).astype('int8')\n",
    "        test_df['prior_question_elapsed_time'].fillna(prior_question_elapsed_time_mean, inplace = True)\n",
    "        test_df = pd.merge(test_df, questions_df[['question_id', 'part']], left_on = 'content_id', right_on = 'question_id', how = 'left')\n",
    "        test_df[TARGET] = 0\n",
    "        test_df = add_features(test_df, update = False)\n",
    "        test_df[TARGET] =  model.predict(test_df[FEATURES])\n",
    "        set_predict(test_df[['row_id', TARGET]])\n",
    "        \n",
    "    print('Job Done')\n",
    "    "
   ],
   "execution_count": 2,
   "outputs": [
    {
     "output_type": "stream",
     "text": "User feature calculation started...\n\n\n",
     "name": "stdout"
    },
    {
     "output_type": "error",
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-2-be30f2b85d14>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m    320\u001B[0m     \u001B[0mprint\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m'Job Done'\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    321\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 322\u001B[0;31m \u001B[0mtrain\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mvalid\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mquestions_df\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mprior_question_elapsed_time_mean\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mfeatures_dicts\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mread_and_preprocess\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mfeature_engineering\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;32mTrue\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    323\u001B[0m \u001B[0mTARGET\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mFEATURES\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mmodel\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mtrain_and_evaluate\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mtrain\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mvalid\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mfeature_engineering\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;32mTrue\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    324\u001B[0m \u001B[0minference\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mTARGET\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mFEATURES\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mmodel\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mquestions_df\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mprior_question_elapsed_time_mean\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mfeatures_dicts\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m<ipython-input-2-be30f2b85d14>\u001B[0m in \u001B[0;36mread_and_preprocess\u001B[0;34m(feature_engineering)\u001B[0m\n\u001B[1;32m    199\u001B[0m     \u001B[0mprint\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m'User feature calculation started...'\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    200\u001B[0m     \u001B[0mprint\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m'\\n'\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 201\u001B[0;31m     \u001B[0mtrain\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0madd_features\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mtrain\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0manswered_correctly_u_count\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0manswered_correctly_u_sum\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0melapsed_time_u_sum\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mexplanation_u_sum\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mtimestamp_u\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mtimestamp_u_incorrect\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0manswered_correctly_q_count\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0manswered_correctly_q_sum\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0melapsed_time_q_sum\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mexplanation_q_sum\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0manswered_correctly_uq\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    202\u001B[0m     \u001B[0mvalid\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0madd_features\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mvalid\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0manswered_correctly_u_count\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0manswered_correctly_u_sum\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0melapsed_time_u_sum\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mexplanation_u_sum\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mtimestamp_u\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mtimestamp_u_incorrect\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0manswered_correctly_q_count\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0manswered_correctly_q_sum\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0melapsed_time_q_sum\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mexplanation_q_sum\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0manswered_correctly_uq\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    203\u001B[0m     \u001B[0mgc\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mcollect\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m<ipython-input-2-be30f2b85d14>\u001B[0m in \u001B[0;36madd_features\u001B[0;34m(df, answered_correctly_u_count, answered_correctly_u_sum, elapsed_time_u_sum, explanation_u_sum, timestamp_u, timestamp_u_incorrect, answered_correctly_q_count, answered_correctly_q_sum, elapsed_time_q_sum, explanation_q_sum, answered_correctly_uq, update)\u001B[0m\n\u001B[1;32m     39\u001B[0m             \u001B[0manswered_correctly_u_avg\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mnum\u001B[0m\u001B[0;34m]\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0manswered_correctly_u_sum\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mrow\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;36m0\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m]\u001B[0m \u001B[0;34m/\u001B[0m \u001B[0manswered_correctly_u_count\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mrow\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;36m0\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     40\u001B[0m             \u001B[0melapsed_time_u_avg\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mnum\u001B[0m\u001B[0;34m]\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0melapsed_time_u_sum\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mrow\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;36m0\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m]\u001B[0m \u001B[0;34m/\u001B[0m \u001B[0manswered_correctly_u_count\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mrow\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;36m0\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 41\u001B[0;31m             \u001B[0mexplanation_u_avg\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mnum\u001B[0m\u001B[0;34m]\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mexplanation_u_sum\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mrow\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;36m0\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m]\u001B[0m \u001B[0;34m/\u001B[0m \u001B[0manswered_correctly_u_count\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mrow\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;36m0\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     42\u001B[0m         \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     43\u001B[0m             \u001B[0manswered_correctly_u_avg\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mnum\u001B[0m\u001B[0;34m]\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mnp\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mnan\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ]
  },
  {
   "metadata": {
    "trusted": true
   },
   "cell_type": "code",
   "source": [
    "train, valid, questions_df, prior_question_elapsed_time_mean, features_dicts = read_and_preprocess(feature_engineering = True)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {
    "trusted": true
   },
   "cell_type": "code",
   "source": [
    "TARGET, FEATURES, model = train_and_evaluate(train, valid)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {
    "trusted": true
   },
   "cell_type": "code",
   "source": [
    "# inference(TARGET, FEATURES, model, questions_df, prior_question_elapsed_time_mean, features_dicts)"
   ],
   "execution_count": null,
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  },
  "language_info": {
   "name": "python",
   "version": "3.7.6",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}